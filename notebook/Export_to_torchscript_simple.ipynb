{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "export_to_torchscript_simple.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZMn7SVT7Cmf"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57nRmgNQ7Bey",
        "outputId": "ff0c03e3-abd1-42a3-9f24-da898e2e918d"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.8.1+cu101 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_FkqaYXcN9H",
        "outputId": "6b34fae9-76c0-4050-dfd2-95eb95e7d720"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqfpuOZKGnA1",
        "outputId": "686b5e14-9a9e-45e0-99ad-b4099527c80a"
      },
      "source": [
        "%env W=/content/drive/MyDrive/SistemiDigitali/pesi/GIT_wfilters/GIT_wfilters_b32i640.pt\n",
        "%env ISIZE=640"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: W=/content/drive/MyDrive/SistemiDigitali/pesi/GIT_wfilters/GIT_wfilters_b32i640.pt\n",
            "env: ISIZE=640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suwvARqBZtJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b456ca4-336f-43df-a6da-55c055a865bb"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# \"model.model[-1].export = not opt.grid\" a \"model.model[-1].export = False\" '\n",
        "#\"from torch.utils.mobile_optimizer import optimize_for_mobile\" \"ts = optimize_for_mobile(ts)\"'\n",
        "\n",
        "\n",
        "cd /content/yolov5\n",
        "python models/export.py  --weights \"$W\" --img-size $ISIZE #--optimize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Aggiungere queste righe al file export.py\n",
            "riga 58 da \"model.model[-1].export = not opt.grid\" a \"model.model[-1].export = False\" \n",
            "tra riga 65 e 66 vanno inserite \"from torch.utils.mobile_optimizer import optimize_for_mobile\" \"ts = optimize_for_mobile(ts)\"\n",
            "\n",
            "Namespace(batch_size=1, device='cpu', dynamic=False, half=False, img_size=[640, 640], inplace=False, optimize=True, simplify=False, train=False, weights='/content/drive/MyDrive/SistemiDigitali/pesi/GIT_wfilters/GIT_wfilters_b32i640.pt')\n",
            "YOLOv5 ðŸš€ v5.0-81-gabfcf9e torch 1.8.1+cu101 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 224 layers, 7062001 parameters, 0 gradients, 16.4 GFLOPS\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/drive/MyDrive/SistemiDigitali/pesi/GIT_wfilters/GIT_wfilters_b32i640.pt (14.4 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 1.8.1+cu101...\n",
            "/content/yolov5/models/yolo.py:51: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n",
            "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success, saved as /content/drive/MyDrive/SistemiDigitali/pesi/GIT_wfilters/GIT_wfilters_b32i640.torchscript.pt (28.4 MB)\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export failure: No module named 'onnx'\n",
            "\u001b[34m\u001b[1mCoreML:\u001b[0m export failure: No module named 'coremltools'\n",
            "\n",
            "Export complete (5.61s). Visualize with https://github.com/lutzroeder/netron.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyTeZyZuQnZi"
      },
      "source": [
        "%%shell\n",
        "du -h *.pt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}